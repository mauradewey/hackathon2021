{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "GP model for NOAA AI 2021 Hackathon 'Climate Bench'.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Environment setup\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import xarray as xr\r\n",
    "from eofs.xarray import Eof\r\n",
    "from esem import gp_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utilities for normalizing the emissions data\r\n",
    "min_co2 = 0.\r\n",
    "max_co2 = 2400\r\n",
    "def normalize_co2(data):\r\n",
    "    return data / max_co2\r\n",
    "\r\n",
    "def un_normalize_co2(data):\r\n",
    "    return data * max_co2\r\n",
    "\r\n",
    "min_ch4 = 0.\r\n",
    "max_ch4 = 0.6\r\n",
    "def normalize_ch4(data):\r\n",
    "    return data / max_ch4\r\n",
    "\r\n",
    "def un_normalize_ch4(data):\r\n",
    "    return data * max_ch4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plot inputs\r\n",
    "data_path = \"C:/Users/maura/OneDrive/Documents/iMIRACLI/hackathon2021/\"\r\n",
    "from glob import glob\r\n",
    "\r\n",
    "inputs = glob(data_path + \"inputs_s*.nc\")\r\n",
    "SECONDS_IN_YEAR = 60*60*24*365 #s\r\n",
    "\r\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,12))\r\n",
    "\r\n",
    "for input in inputs:\r\n",
    "    label=input.split('_')[1][:-3]\r\n",
    "    X = xr.open_dataset(input)\r\n",
    "    x = range(2015, 2101)\r\n",
    "\r\n",
    "    weights = np.cos(np.deg2rad(X.latitude))\r\n",
    "    \r\n",
    "    axes[0, 0].plot(x, X['CO2'].data, label=label)\r\n",
    "    axes[0, 0].set_ylabel(\"Cumulative anthropogenic CO2 \\nemissions since 1850 (GtCO2)\")\r\n",
    "    axes[0, 1].plot(x, X['CH4'].data, label=label)\r\n",
    "    axes[0, 1].set_ylabel(\"Anthropogenic CH4 \\nemissions (GtCH4 / year)\")\r\n",
    "    # FIXME: Not sure where this factor of 1000 comes from...! Maybe the CEDS data is really g/m-2/s?\r\n",
    "    axes[1, 0].plot(x, X['SO2'].weighted(weights).sum(['latitude', 'longitude']).data*SECONDS_IN_YEAR*1e-9, label=label)\r\n",
    "    axes[1, 0].set_ylabel(\"Anthropogenic SO2 \\nemissions (GtSO2 / year)\")\r\n",
    "    axes[1, 1].plot(x, X['BC'].weighted(weights).sum(['latitude', 'longitude']).data*SECONDS_IN_YEAR*1e-9, label=label)\r\n",
    "    axes[1, 1].set_ylabel(\"Anthropogenic BC \\nemissions (GtBC / year)\")\r\n",
    "\r\n",
    "axes[0, 0].set_title('CO2')\r\n",
    "axes[0, 1].set_title('CH4')\r\n",
    "axes[1, 0].set_title('SO2')\r\n",
    "axes[1, 1].set_title('BC')\r\n",
    "axes[0, 0].legend()\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Set up training data\r\n",
    "\r\n",
    "#adapted from Kai and Theorodore:\r\n",
    "def cube_to_vector(data_array):\r\n",
    "    \"transforms 3 dimensional xarray (time, lat, lon) to 2 dimensional numpy array (time, lat*lon)\"\r\n",
    "    time_range = data_array[\"time\"].shape[0]\r\n",
    "    \r\n",
    "    return data_array.values.reshape(time_range,96*144)\r\n",
    "\r\n",
    "\r\n",
    "def moving_average(a, n) :\r\n",
    "    ret = np.cumsum(a, dtype=float)\r\n",
    "    ret[n:] = ret[n:] - ret[:-n]\r\n",
    "    return ret[n - 1:] / n\r\n",
    "\r\n",
    "def create_predictor_data(data_set, n_eofs=5):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        data_set (str): name of dataset\r\n",
    "        n_eofs (int): number of eofs to create for aerosol variables\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "    X = xr.open_dataset(data_path + \"inputs_{}.nc\".format(data_set)).compute()\r\n",
    "    \r\n",
    "    if data_set == \"hist-aer\":\r\n",
    "        X = X.rename_vars({\"CO4\":\"CO2\"})\r\n",
    "        X = X.sel(time=slice(1850,2100))\r\n",
    "\r\n",
    "    if data_set == \"hist-GHG\":\r\n",
    "        X = X.sel(time=slice(1850,2014))\r\n",
    "    \r\n",
    "    if \"ssp\" in data_set or data_set == \"hist-aer\":\r\n",
    "        # Create an EOF solver to do the EOF analysis. Square-root of cosine of\r\n",
    "        # latitude weights are applied before the computation of EOFs.\r\n",
    "        bc_solver = Eof(X['BC'])\r\n",
    "\r\n",
    "        # Retrieve the leading EOF, expressed as the correlation between the leading\r\n",
    "        # PC time series and the input SST anomalies at each grid point, and the\r\n",
    "        # leading PC time series itself.\r\n",
    "        bc_eofs = bc_solver.eofsAsCorrelation(neofs=n_eofs)\r\n",
    "        bc_pcs = bc_solver.pcs(npcs=n_eofs, pcscaling=1)\r\n",
    "\r\n",
    "        # Create an EOF solver to do the EOF analysis. Square-root of cosine of\r\n",
    "        # latitude weights are applied before the computation of EOFs.\r\n",
    "        so2_solver = Eof(X['SO2'])\r\n",
    "\r\n",
    "        # Retrieve the leading EOF, expressed as the correlation between the leading\r\n",
    "        # PC time series and the input SST anomalies at each grid point, and the\r\n",
    "        # leading PC time series itself.\r\n",
    "        so2_eofs = so2_solver.eofsAsCorrelation(neofs=n_eofs)\r\n",
    "        so2_pcs = so2_solver.pcs(npcs=n_eofs, pcscaling=1)\r\n",
    "\r\n",
    "        # Convert the Principle Components of the aerosol emissions (calculated above) in to Pandas DataFrames\r\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\r\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\r\n",
    "\r\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\r\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\r\n",
    "    else:\r\n",
    "        # all values are zero, fill up eofs so we have same inputs as for other datasets\r\n",
    "        timesteps = X[\"time\"].shape[0]\r\n",
    "        zeros = np.zeros(shape=(timesteps, n_eofs))\r\n",
    "        bc_df = pd.DataFrame(zeros, columns=[f\"BC_{i}\" for i in range(n_eofs)],index=X[\"BC\"].coords['time'].data)\r\n",
    "        so2_df = pd.DataFrame(zeros, columns=[f\"SO2_{i}\" for i in range(n_eofs)],index=X[\"BC\"].coords['time'].data)\r\n",
    "\r\n",
    "    # Bring the emissions data back together again and normalise\r\n",
    "    inputs = pd.DataFrame({\r\n",
    "        \"CO2\": normalize_co2(X[\"CO2\"].data),\r\n",
    "        \"CH4\": normalize_ch4(X[\"CH4\"].data)\r\n",
    "    }, index=X[\"CO2\"].coords['time'].data)\r\n",
    "\r\n",
    "    #5-year running mean of CO2\r\n",
    "    co2_1 = normalize_co2(X[\"CO2\"].data)\r\n",
    "    co2_5yr=moving_average(co2_1,5)\r\n",
    "    nan_array = np.empty((1,4))\r\n",
    "    nan_array[:] = np.NaN\r\n",
    "    co2_5yr = np.append(nan_array, co2_5yr)\r\n",
    "    \r\n",
    "    co2_5 = pd.DataFrame({\"CO2_5YR\": co2_5yr}, index=X[\"CO2\"].coords['time'].data)\r\n",
    "    \r\n",
    "    # Combine with aerosol EOFs\r\n",
    "    inputs=pd.concat([inputs, bc_df, so2_df, co2_5], axis=1)\r\n",
    "    \r\n",
    "    # replace nan values with 0\r\n",
    "    inputs = inputs.replace(np.nan, 0)\r\n",
    "    \r\n",
    "    return inputs\r\n",
    "\r\n",
    "def create_predicatand_data(data_set):\r\n",
    "    Y = xr.open_dataset(data_path + \"outputs_{}.nc\".format(data_set)).mean(\"member\")\r\n",
    "    if data_set == \"hist-aer\":\r\n",
    "        Y = Y.sel(time=slice(1850,2100))\r\n",
    "    # Convert the precip values to mm/day\r\n",
    "    Y[\"pr\"] *= 86400\r\n",
    "    Y[\"pr90\"] *= 86400\r\n",
    "    return Y\r\n",
    "\r\n",
    "def create_predicatand_data(data_set):\r\n",
    "    Y = xr.open_dataset(data_path + \"outputs_{}.nc\".format(data_set)).mean(\"member\")\r\n",
    "    # Convert the precip values to mm/day\r\n",
    "    Y[\"pr\"] *= 86400\r\n",
    "    Y[\"pr90\"] *= 86400\r\n",
    "        \r\n",
    "    return Y\r\n",
    "\r\n",
    "files = [\"ssp126\",\"ssp585\",\"historical\",\"1pctCO2\",\"hist-GHG\",\"hist-aer\"]\r\n",
    "\r\n",
    "# create training data\r\n",
    "X_train = pd.concat([create_predictor_data(file) for file in files])\r\n",
    "y_train_tas = np.vstack((cube_to_vector(create_predicatand_data(file)[\"tas\"]) for file in files))\r\n",
    "y_train_pr = np.vstack((cube_to_vector(create_predicatand_data(file)[\"pr\"]) for file in files))\r\n",
    "y_train_dtf = np.vstack((cube_to_vector(create_predicatand_data(file)[\"diurnal_temperature_range\"]) for file in files))\r\n",
    "y_train_pr90 = np.vstack((cube_to_vector(create_predicatand_data(file)[\"pr90\"]) for file in files)) \r\n",
    "\r\n",
    "files2 = [\"ssp370\"]\r\n",
    "\r\n",
    "# create test data\r\n",
    "X_test = pd.concat([create_predictor_data(file) for file in files2])\r\n",
    "test_Y = xr.open_dataset(data_path + 'outputs_ssp370.nc').compute()\r\n",
    "\r\n",
    "tas_truth = test_Y[\"tas\"].mean('member')\r\n",
    "pr_truth = test_Y[\"pr\"].mean('member') * 86400\r\n",
    "pr90_truth = test_Y[\"pr90\"].mean('member') * 86400\r\n",
    "dtr_truth = test_Y[\"diurnal_temperature_range\"].mean('member')\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pick kernel and train\r\n",
    "kernel = ['Linear','RBF']\r\n",
    "from esem.data_processors import Whiten, Normalise\r\n",
    "\r\n",
    "tas_gp = gp_model(X_train, y_train_tas,kernel=kernel,kernel_op='mul')\r\n",
    "tas_gp.train()\r\n",
    "\r\n",
    "#pr_gp = gp_model(X_train, y_train_pr, kernel=kernel, kernel_op='mul')\r\n",
    "#pr_gp.train()\r\n",
    "\r\n",
    "#dtr_gp = gp_model(X_train, y_train_dtr, kernel=kernel, kernel_op='mul')\r\n",
    "#dtr_gp.train()\r\n",
    "\r\n",
    "#pr90_gp = gp_model(X_train, y_train_pr90, kernel=kernel, kernel_op='mul')\r\n",
    "#pr90_gp.train()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mod_tas, _ = tas_gp.predict(X_test)\r\n",
    "\r\n",
    "#put output back into pd.DataFrame format for calculating RMSE/plotting\r\n",
    "m_tas = np.reshape(mod_tas, [86, 96, 144])\r\n",
    "m_tas_data = xr.DataArray(m_tas, dims = tas_truth.dims)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#test (currently on ssp370 because I don't have the ssp245 output. Test data created above with training data.)\r\n",
    "\r\n",
    "def get_rmse(truth, pred):\r\n",
    "    weights = np.cos(np.deg2rad(truth.lat))\r\n",
    "    return np.sqrt(((truth-pred)**2).weighted(weights).mean(['lat', 'lon'])).data\r\n",
    "\r\n",
    "print(f\"RMSE at 2015: {get_rmse(tas_truth[0], m_tas_data[0])}\")\r\n",
    "print(f\"RMSE at 2050: {get_rmse(tas_truth[35], m_tas_data[35])}\")\r\n",
    "print(f\"RMSE at 2100: {get_rmse(tas_truth[85], m_tas_data[85])}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE at 2015: 0.44106167100903215\n",
      "RMSE at 2050: 0.3950025962795656\n",
      "RMSE at 2100: 2.635233842390099\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}